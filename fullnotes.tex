\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{geometry}
\geometry{
  paper=a4paper,
  margin=1in,
}

\usepackage{amsmath,amsthm,amsfonts}
\usepackage{url}
\usepackage{hyperref}

\usepackage{graphicx}

\usepackage{mathtools}
\usepackage{centernot}
\usepackage{mathrsfs}

\usepackage[shortlabels]{enumitem}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}


\usepackage{tcolorbox}

\newtheorem{exmp}{Example}[section]
\newtheorem*{sol}{Solution}

\numberwithin{equation}{section}

\newcommand*{\colorboxed}{}
\def\colorboxed#1#{%
  \colorboxedAux{#1}%
}
\newcommand*{\colorboxedAux}[3]{%
  % #1: optional argument for color model
  % #2: color specification
  % #3: formula
  \begingroup
    \colorlet{cb@saved}{.}%
    \color#1{#2}%
    \boxed{%
      \color{cb@saved}%
      #3%
    }%
  \endgroup
}


% \setcounter{section}{-1}

\title{Mathematical Methods}
\author{Iain McWhinnie}
\date{December 2022}

\begin{document}

\maketitle

\tableofcontents

\pagebreak
\section{Second Order Linear Differential Equations}

\subsection{Introduction}

In this chapter we aim to find solutions to 2nd order linear differential equations.

\begin{definition}
A general \textbf{second order ODE} has the form
\[
y'' = f(t,y,y')
\]
where $y$ is the unknown function and $f: \mathbb{R}^3 \mapsto \mathbb{R}$ is given.


The \textbf{order} of the differential equation  is the highest derivative appearing in the ODE. The equation is \textbf{linear} if the function $f$ is linear in both arguments $y$ and $y'$ i.e.,
\[
f(t,y,y') = a(t)y + b(t)y' + c(t).
\]

\end{definition}

\begin{definition}
    A second order \textbf{linear} differential equation in the unknown $y$ is
    \begin{equation}
        a(t)y'' + b(t)y' + c(t) = h(t)
    \end{equation}
    where $a,b,c,h: I \mapsto \mathbb{R}$ are given functions on the interval $I \subseteq \mathbb{R}$. 
    
    The equation is called \textbf{homogeneous} if the source term $h(t)=0$ for all $t\in I$. The equation is called \textbf{constant coefficient} if $a(t), b(t)$ and $c(t)$ are constants otherwise the equation is called \textbf{variable coefficient}.

\end{definition}

\begin{tcolorbox}
\begin{exmp}\hfill
\begin{enumerate}[(a)]
    \item A second order linear homogeneous constant coefficient equation is
    $$ y'' + 5y' + 6y = 0.$$
    \item A second order linear inhomogeneous constant coefficient equation is
    $$ y'' - 3y' + y = \cos(3t).$$
    \item A second order linear inhomogeneous variable coefficient equation is
    $$ y'' - 2ty' - \ln(t)y = e^{3t}.$$
    \item Newton's second law of motion for a point particle of mass $m$, moving in one space dimensions under a force $f$ is given by
    $$ my'' = f(t)$$
    ($\text{force} = \text{mass} \times \text{acceleration}$).
\end{enumerate}
\end{exmp}
\end{tcolorbox}

\subsection{Initial Value Problems}

\begin{thm}[Existence and Uniqueness]\label{thm:Existenceanduniqueness}
If the functions $p(t),q(t),f(t):I \mapsto \mathbb{R}$ are continuous on a closed interval $I \subseteq \mathbb{R}$ and $t_0 \in I$ and $b_0,b_1 \in \mathbb{R}$ are constants, then there \textbf{exists} a \textbf{unique} solution $y: I \mapsto \mathbb{R}$ to the \textbf{initial value problem} (IVP)
\begin{equation}\label{eq:linearIVP}
\left.
\begin{aligned}
    y'' + p(t)y' + q(t)y = f(t) \\
    y(t_0) = b_0, \quad y'(t_0) = b_1 \\
\end{aligned}\quad\right\rbrace
\end{equation}
\end{thm}

\begin{proof}
    Omitted. It is actually an application of the contraction mapping theorem from metric spaces.
\end{proof}

\begin{figure}[h]
    \centering
    \includegraphics{images/Ivpgraph.png}
    \caption{An initial value problem.}
    \label{fig:ivpgraph}
\end{figure}

\begin{tcolorbox}
\begin{exmp}
Find the largest interval $I \in \mathbb{R}$ such that there exists a unique solution to the IVP:
\[
\begin{aligned}
    (t-1)y'' - 3ty' + 4y = t(t-1) \\
    y(-2) = 2, \quad y'(-2) = 1 \\
\end{aligned}
\]
\end{exmp}
\begin{sol}
    Rewriting the ODE in the form of Theorem \ref{thm:Existenceanduniqueness} we have
    \[
    y'' - \frac{3t}{t-1}y' + \frac{4}{t-1}y = t.
    \]
    The intervals where $\frac{3t}{t-1}, \frac{4}{t-1}$ and $t$ are all continuous are
    \[
        I_1 = (-\infty, 1) \quad \text{and} \quad I_2 = (1, \infty).
    \]
    Since the initial condition lies in the interval $I_1$ the solution domain is
    \[
        I_1 = (-\infty, 1).
    \]
\end{sol}
\end{tcolorbox}

We will look at an example of a homogeneous equation with constant coefficients

\begin{tcolorbox}
\begin{exmp}
    Solve $y''+5y' + 6y = 0$ with $y(0)=2$ and $y'(0)=3$.
\end{exmp}
\begin{sol}
    We try a solution of the form $y(t)=\exp(\lambda t)$ to derive the auxiliary equation
    \[
    \lambda^2 + 5\lambda + 6 =9
    \]
    which factors to give $(\lambda + 2)(\lambda + 3) = 0$. Hence $\lambda = -2$ or $\lambda = -3$.

    The general solution is then
    \[
        y(t) = c_1 e^{-2t} + c_2 e^{-3t},  
    \qquad c_1,c_2 \in \mathbb{R}.
    \]
    Applying the initial conditions we find that
    \[
    \left.
    \begin{aligned}
        y(0) = 2 = c_1 + c_2 \\
        y'(0) = 3 = 2c_1 - 3c_2 \\
    \end{aligned}
    \quad\right\rbrace
    \]
    Hence $c_1 = 9$ and $c_2 = -7$ so the unique solution to the IVP is
    \[
        y(t) = 9e^{-2t} - 7e^{-3t}.
    \]
\end{sol}
\end{tcolorbox}

This leaves us with a few questions
\begin{enumerate}[(i)]
    \item Why did we add the two solutions together to get the general one?
    \item Why is this the general solution?
    \item What would the solution to the more general ODE \eqref{eq:linearIVP} look like?
\end{enumerate}

To answer these questions it is useful to introduce the definition of an operator, we will do this in the next section.

\subsection{Linear Operators}

A function $f: \mathbb{R} \mapsto \mathbb{R}$ takes a number as an input and produces another number as an output
\[
    x \ \xlongrightarrow{\quad\textit{f}\quad} \ f(x)
\]
i.e.,
\[
    \text{number}\ \xlongrightarrow{\quad\textit{f}\quad} \ \text{another number}.
\]
A simple example is $f(x) = x^2$ which maps
\[
    x \ \xlongrightarrow{\quad\textit{f}\quad}  \ x^2.
\]
One can thing of differentiating in a similar way but with the inputs and outputs as functions rather than numbers, i.e.,
\[
    \text{function}\ \xlongrightarrow{\text{operator}}\ \text{another function}.
\]
For example,
\[
    y(t)\ \xlongrightarrow{\quad\textit{L}\quad} \ y'' + y
\]
here $L = \frac{d^2}{dt^2} + 1$. $L$ is called an \textbf{operator} or \textbf{functional}.

\begin{tcolorbox}
\begin{exmp}
    Let $L = \frac{d}{dx} + x^2$, then $L$ sends the function $y(x)$ to the function $\frac{dy}{dx} + x^2y$.

    We use square brackets $L[y]$ to denote the output of the operator so
    \[
        L[y] = \frac{dy}{dx} + x^2y.
    \]
    The new function $L[y]$ can be evaluated at a number $x$, this is denoted by
    $$ L[y(x)] \quad \text{or} \quad L[y](x).$$
    So for example
    \begin{align*}
        L\left[\sin(x)\right] &= \frac{d}{dx}\left( \sin x \right) + x^2 \sin x \\
        &= \cos x + x^2 \sin x. \\
    \end{align*}
\end{exmp}
\end{tcolorbox}

Let us consider the operator
\begin{equation*}
    L[y] = y'' + p(t)y' + q(t)y
\end{equation*}
then equation \eqref{eq:linearIVP} can be written as $L[y] = f$.

\bigskip
A particularly important type of operators are linear operators.

\begin{definition}
    An operator $L$ is called a \textbf{linear operator} if for every pair of function $y_1$ and $y_2$ and constants $c_1,c_2$ we have that
    \[
        L\left[ c_1y_1 + c_2y_2 \right] = c_1 L[y_1] + c_2 L[y_2].
    \]
\end{definition}

\begin{tcolorbox}
\begin{exmp}
    Show that the operator $L = \frac{d^2}{dx^2} + p(t)\frac{d}{dt} + q(t)$ is linear.
\end{exmp}
\begin{sol}
    \begin{align*}
        L[c_1y_1 + c_2y_2] &= \big(c_1y_1 + c_2y_2\big)'' + p(t)\big(c_1y_1 + c_2y_2\big)' + q(t)\big(c_1y_1 + c_2y_2\big) \\
        &= \big(c_1y_1''+p(t)c_1y_1' + q(t)c_1y_1\big) + \big(c_2y_2'' + p(t)c_2y_2' + q(t)c_2y_2\big)\\
        &= c_1 \big( y_1'' + p(t)y_1' + q(t)y_1 \big) + c_2 \big( y_2'' + p(t)y_2' + q(t)y_2 \big) \\
        &= c_1 L[y_1] + c_2 L[y_2] \\
    \end{align*}
    hence $L$ is a linear operator.
\end{sol}

\end{tcolorbox}
\begin{tcolorbox}

\begin{exmp}
    Show that the operator $L[y] = \left(\frac{dy}{dx}\right)^2$ is \textbf{not} linear.
\end{exmp}
\begin{sol}
    \begin{align*}
        L[y_1 + y_2] &= \left(\frac{d(y_1 + y_2)}{dx}\right)^2 = \left( \frac{dy_1}{dx} + \frac{dy_2}{dx} \right)^2 \\
        &= \left(\frac{dy_1}{dx}\right)^2 + 2\frac{dy_1}{dx}\frac{dy_2}{dx} + \left( \frac{dy_2}{dx} \right)^2 \\
        &= L[y_1] + L[y_2] + 2\sqrt{L[y_1]L[y_2]} \\
        & \neq L[y_1] + L[y_2] \\
    \end{align*}
    so $L$ is not a linear operator.
\end{sol}
\end{tcolorbox}
\bigskip
The linearity of an operator $L$ translates to into the superposition property (adding solutions) of the solutions to the homogeneous equation $L[y] = 0$.

\begin{thm}[The Principle of Superposition]
    If $L$ is a linear operator and $y_1,y_2$ are solutions of the homogenenous equations
    \[
        L[y_1] = 0, \quad L[y_2] = 0,
    \]
    then for every constant $c_1,c_2$ we have
    \[
        L[c_1y_1 + c_2y_2] = 0.
    \]
\end{thm}
\begin{proof}
    We verify that $y=c_1y_1 + c_2y_2$ satisfies $L[y] = 0$ for every constant $c_1,c_2.$ We have that
    \begin{align*}
        L[y] &= L[c_1y_1 + c_2y_2] \\
        &= c_1L[y_1] + c_2L[y_2] \\
        &= c_1 \cdot 0 + c_2 \cdot 0 \\
        &= 0
    \end{align*}
    since $L$ is a linear operator.
\end{proof}

\subsection{Linear Dependence of Functions and Wronskians}\label{section:LinearDependence&Wronskians}

We are going to digress away from solutions to ODEs in this section and think about linear dependance of functions.

\begin{definition}
    Two continuous functions $y_1, y_2: I \mapsto \mathbb{R}$ are called \textbf{linearly dependent} on the interval $I \subseteq \mathbb{R}$, if there exists constants $c_1, c_2 \in \mathbb{R}$ not both zero such that for all $t\in I$ we have
    \[
        c_1y_1(t) + c_2y_2(t) = 0.
    \]
    On the contrary, $y_1,y_2$ are \textbf{linearly independent} on the interval $I$ if they are \textbf{not} linearly dependent, that is, the only constants $c_1,c_2$ that for all $t\in I$ satisfy $c_1y_1(t) + c_2y_2(t) = 0$ are the constants $c_1=c_2=0.$
\end{definition}

\begin{tcolorbox}
    \begin{exmp}
        Which of the following pairs of functions are linearly independent?
        \begin{enumerate}[(a)]
            \item $\sin t$ and $\cos(t-\pi/2)$,
            \item $e^t$ and $e^{2t}$.
        \end{enumerate}
    \end{exmp}
    \begin{sol}
        \begin{enumerate}[(a)]
            \item Consider $c_1\sin(t) + c_2 \cos(t-\pi/2) = 0, \quad c_1,c_2 \in \mathbb{R}$. Now by looking at the triangle in Figure \ref{fig:triangle} we can see that $\sin(t) = \cos(t-\pi/2).$

            Therefore by choosing $c_1=-c_2$ we have
            \[
                c_1\sin(t) + c_2 \cos(t-\pi/2) = 0.
            \]
            Thus $c_1,c_2 \neq 0$ and the two function are linearly dependent.
            \item Consider $c_1e^t + c_2e^{2t} = 0, \quad c_1,c_2 \in \mathbb{R}$. Differentiating gives $c_1e^t + 2c_2e^{2t} = 0$. Subtracting one equation from the other gives $c_2e^{2t} = 0$ so $c_2 = 0$. Thus $c_1 = 0$ and therefore the functions are linearly independent.
        \end{enumerate}
    \end{sol}
\end{tcolorbox}

\begin{figure}[htb]
    \centering
    \includegraphics{images/traingle.png}
    \caption{Angles in a right angled triangle}
    \label{fig:triangle}
\end{figure}

\begin{tcolorbox}
\begin{exmp}
    If either function $y_1$ or $y_2$ is the zero function then $y_1,y_2$ are linearly independent. We can show this as follows, let $y_1(t) = 0$ for instance then
    \[
        c_1y_1(t) + 0 \cdot y_2(t) = c_1 \cdot 0 = 0.
    \]
    This is true for any value of $c_2$ therefore there exists $c_1,c_2$ not both zero such that $c_1y_1 + c_2y_2 = 0$.
\end{exmp}
\end{tcolorbox}

\bigskip
\begin{definition}[The Wronskian function]
    The \textbf{Wronskian} of the differentiable functions $y_1,y_2$ is the function
    \[
        W(y_1,y_2) = \left\vert\ \begin{matrix}
            y_1 & y_2\\
            y_1' & y_2'\\
        \end{matrix}\ \right\vert
        = y_1(t)y_2'(t) - y_1'(t)y_2(t).
    \]
\end{definition}

\begin{tcolorbox}
    \begin{exmp}
        Find the Wronskian of the functions
        \begin{enumerate}[(a)]
            \item $y_1(t) = \sin(t)$ and $y_2(t) = 2\sin(t)$,
            \item $y_2(t) = \sin(t)$ and $y_2(t) = t\sin(t)$.
        \end{enumerate}
    \end{exmp}
    \begin{sol}
        \begin{enumerate}[(a)]\hfill
            \item 
            \[
            \begin{aligned}
                W(y_1, y_2) &= \left\vert\ \begin{matrix}
                    y_1(t) & y_2(t)\\
                    y_1'(t) & y_2'(t)\\
                    \end{matrix}\ \right\vert
                = \left\vert\ \begin{matrix}
                    \sin(t) & 2\sin(t)\\
                    \cos(t)& 2\cos(t)\\
                    \end{matrix}\ \right\vert \\
                &= \sin(t)2\cos(t) - \cos(t)2\sin(t) = 0 \\
            \end{aligned}
            \]
            (Note that $y_1,y_2$ are linearly dependent.)
            \item 
            \[ W(y_1, y_2) = \left\vert\ \begin{matrix}
                    y_1(t) & y_2(t)\\
                    y_1'(t) & y_2'(t)\\
                    \end{matrix}\ \right\vert
                = \left\vert\ \begin{matrix}
                    \sin(t) & t\sin(t)\\
                    \cos(t)& \sin(t) + t\cos(t)\\
                    \end{matrix}\ \right\vert
                = \sin^2(t) \\
            \]
            (Note that $y_1,y_2$ are linearly independent.)
        \end{enumerate}
    \end{sol}
\end{tcolorbox}

\begin{thm}
    If the Wronskian $W(y_1(t_0), y_2(t_0)) \neq 0$ at a single point $t_0 \in I$ then the functions $y_1,y_2: I \mapsto \mathbb{R}$ are linearly independent.
\end{thm}

\begin{proof}
    Assume $y_1,y_2$ are linearly dependent then $\exists c \neq 0$ such that $y_1=cy_2$, so
    \[
    W(y_1, y_2) = y_1(t)y_2'(t) - y_1'(t)y_2(t) = (cy_2(t))y_2'(t) - cy_2'(t)y_2(t) = 0 \quad \forall t \in I.
    \]
    This is a contradiction as there exists a $t_0 \in I$ such that $W(y_1,y_2)\neq 0$ so $y_1,y_2$ are linearly independent.
\end{proof}

In the steps of the previous proof we showed that if $f$ and $g$ are linearly dependent on $I$ then $W(f,g)=0$ for all $t \in I$. But this is a one way implication so we can have $W(f,g)=0$ for all $t\in I$ and $f,g$ linearly independent.

\begin{tcolorbox}
    \begin{exmp}\label{exmp:WronskianContraEx}
        Let $f(t) = t^2\vert t \vert$, $g(t) = t^3$. Show that $f,g$ are linearly independent on $I=(-1,1)$ but $W(f,g)=0, \; \forall t \in I$.
    \end{exmp}
    \begin{sol}
        \[
        f'(t) = \begin{cases}
            2t^2 + t^2, & t\geq 0 \\
            -2t^2 - t^2, & t < 0 \\
        \end{cases} \quad \text{and} \quad g'(t) = 3t^2.
        \]
        Thus for $c_1,c_2 \in \mathbb{R}$ if $c_1f+c_2g=0$, i.e., $c_1t^2\vert t \vert + c^2 t^3 = 0$ holds for all $t$ then at $t=1/2$ say
        \[
        \frac{c_1}{8} = -\frac{c_2}{8} \implies c_1=-c_2,
        \]
        and at $t=-1/2$ say
        \[
            \frac{c_1}{8} = \frac{c_2}{8} \implies c_1=c_2
        \]
        hence $c_1=c_2=0$ and $f,g$ are linearly independent.

        Now calculating $W(f,g)$ gives
        \[
        \begin{aligned}
        W(f,g) &= \begin{cases}
            t^3 \cdot 3t^2 - 3t^2 \cdot t^3  = 0, & t \geq 0 \\
            -t^3 \cdot 3t^2 + 3t^2 \cdot t^3 = 0, & t < 0 \\
        \end{cases} \\
        &= 0 \quad \forall t \in I.
        \end{aligned}
        \]
    \end{sol}
\end{tcolorbox}

Here is a summary of the implications here:
\begin{enumerate}[(i)]
    \item If $f,g$ are linearly dependent $\implies$ $W(f,g)=0$ for all $t$,
    \item If $W(f,g)=0 \; \forall t \centernot\implies f,g$ are linearly dependent,
    \item Contrapositive of (i). If $W(f,g) \neq 0$ for any $t_0 \in I$ then $f,g$ are linearly independent.
\end{enumerate}

\subsection{Some Theorems About Solutions}

\begin{thm}[Abel's Theorem]
    If $y_1,y_2$ are twice continuously differentiable solutions of
    \[
    L[y] = y'' + p(t)y' + q(t)y = 0
    \]
    where $p(t),q(t)$ are continuous on $I \subseteq \mathbb{R}$ then the Wronskian $W(y_1, y_2)$ satisfies
    \begin{equation}
    \frac{dW}{dt} + p(t)W = 0.
    \label{eq:WronskianODE}
    \end{equation}
    Moreover, for any $t_0 \in I$, the Wronskian $W(y_1, y_2)$ is given by
    \begin{equation}
    \tag{Abel's Formula}
    W(y_1, y_2) = W_0 \exp\left(-\int_{t_0}^{t} p(s) ds \right) 
    \label{eq:AbelsFormula}
    \end{equation}
    where $W_0 = W(y_1(t_0), y_2(t_0))$.
\end{thm}

\begin{proof}
    Firstly,
    \[
    \frac{dW}{dt} = \Big( y_1y_2' - y_1'y_2 \Big)' = y_1y_2'' - y_1''y_2.
    \]
    Since $y_1$ and $y_2$ are solutions, $L[y] = 0$ gives
    \[
        y_1'' = -p(t)y_1' - q(t)y_1
    \]
    and similarly
    \[
        y_2'' = -p(t)y_2' - q(t)y_2.
    \]
    Substitute these in the expression for $\frac{dW}{dt}$ to get
    \begin{align*}
        \frac{dW}{dt} &= y_1\Big( -py_2' - qy_2 \Big) - y_2\Big( -py_1' - qy_1 \Big) \\
        &= -p(y_1y_2' - y_1'y_2) \\
        &= - pW. 
    \end{align*}
    We can solve this equation by separating the variables.
    \begin{align*}
        \int_{t_0}^{t} \frac{1}{W} dW &= - \int_{t_0}^t p(s) ds \\
        \ln\left(W(t)\right) - \ln\left(W(t_0)\right) &= - \int_{t_0}^t p(s) ds \\
        W(y_1, y_2) &= W_0\exp\left( - \int_{t_0}^t p(s) ds \right)
    \end{align*}
    where $W_0 = W(y_1(t_0), y_2(t_0))$.
\end{proof}

\begin{thm}
    Let $y_1,y_2: I \mapsto\mathbb{R}$ both be solutions of $L[y]=0$ on $I$. If there exists a $t_0\in I$ such that $W(y_1(t_0),y_2(t_0)) = 0$ then $y_1$ and $y_2$ are linearly dependent.
\end{thm}

\begin{proof}
    \ref{eq:AbelsFormula} says that if the Wronskian $W(y_1(t_0), y_2(t_0)) = 0$ then $W(y_1,y_2)=0, \; \forall t \in I$.

    If either $y_1$ or $y_2$ is the the zero function then $y_1, y_2$ are linearly dependent. So we will assume that both $y_1,y_2$ are are \textbf{not} identically zero. Assume there exists a $t_1 \in I$ such that $y_1(t_1) \neq 0$. By continuity $y_1$ is non-zero in an open neighbourhood $I_1 \subset I$ of $t_1$ so
    \[
    \frac{W(y_1,y_2)}{y_1^2} = \frac{y_1y_2' - y_1'y_2}{y_1^2} = 0.
    \]
    Hence
    \[
        \left( \frac{y_2}{y_1} \right)' = 0 \implies \frac{y_2}{y_1} = c
    \]
    on $I_1$, where $c\in \mathbb{R}$ is an arbitrary constant. Therefore $y_2=cy_1$, i.e. $y_2$ is proportional to $y_1$ on $I_1$.

    The zero function also satisfies $L[y] = 0, y(t_1)=0, y'(t_1)=0.$ By the existence and uniqueness theorem (Theorem \ref{thm:Existenceanduniqueness}) we must have that $y(t)$ is the zero function on $I$. So $y_1$ is proportional to $y_2$ on $I$ and $y_1,y_2$ are linearly dependent as required.
\end{proof}

\begin{cor}[Contrapositive of above theorem]
    Let $y_1,y_2: I \mapsto \mathbb{R}$ both be solutions of $L[y]=0$ on $I$. If $y_1,y_2$ are linearly independent on $I$ then their Wronskian $W(y_1,y_2)\neq 0$ for all $t\in I$.
\end{cor}

We comment that in an earlier example, Example \ref{exmp:WronskianContraEx}, with 2 linear independent functions which had a zero Wronskian. This corollary does not contradict that example. This just tells us that the 2 functions $f,g$ in the example are not both solutions to some ODE, $y'' + p(t)y' + q(t)y=0$, with $p,q$ continuous on $I$.

\subsection{General Solutions}

\begin{definition}
\begin{enumerate}[(a)]\hfill
    \item
    The functions $y_1$ and $y_2$ are called \textbf{fundamental solutions} of the equation $L[y]=0$ if $L[y_1]=1$ and $L[y_2]=0$ \textbf{and} $y_1,y_2$ are linearly independent.
    \item
    The \textbf{general solution} of the homogeneous second order linear equation $L[y] = 0$ is two parameter family of functions $y_{\text{gen}}$ given by:
    \[
        y_{\text{gen}} = c_1y_1(t) + c_2y_2(t)
    \]
    where the arbitrary constants $c_1$ and $c_2$ are the parameters of the family and $y_1,y_2$ are fundamental solutions of $L[y]=0$.
\end{enumerate}
\end{definition}

\begin{thm}[General Solutions]
If $y_1$ and $y_2$ are linearly independent solutions of the equation $L[y]=0$ on the interval $I \subseteq \mathbb{R}$ where
\[
    L[y] = y'' + p(t)y' + q(t)y
\]
and $p(t),q(t)$ are continuous functions on $I$, then there exists \textbf{unique} constants $c_1,c_2$ such that \textbf{every} solution $y$ of the differential equation $L[y]=0$ can be written as a linear combination
\[
    y(t) = c_1y_1(t) + c_2y_2(t).
\]
\end{thm}
\begin{proof}
We need to show that given any fundamental solution pair $y_1,y_2$ \textbf{any} other solution $y$ to the homogeneous equation $L[y] = 0$ must be a \textbf{unique} linear combination of the fundamental solutions $y(t) = c_1y_1(t) + c_2y_2(t)$ for appropriately chosen constants.

\begin{enumerate}[(i)]
    \item The super position property gives us that any function $y(t) = c_1y_1(t) + c_2y_2(t)$ is a solution of $L[y] = 0$ for every pair of constants $c_1$ and $c_2$.
    \item Given a solution $y$, if there exists $c_1,c_2$ such that $y(t) = c_1y_1(t) + c_2y_2(t)$ holds then these constants are unique. We can show this as follows:

    If there are other constants $\Tilde{c_1}, \Tilde{c_2}$ so that $y(t) = \Tilde{c_1}y_1(t) + \Tilde{c_2}y_2(t)$ then if we subtract this equation from the eqn with $c_1$ and $c_2$ we get
    \[
        0 = (c_1 - \Tilde{c_1}) y_1 + (c_2-\Tilde{c_2})y_2 \\
        \implies c_1-\Tilde{c_1} = 0 \quad \text{ and } \quad c_2-\Tilde{c_2} = 0
    \]
    since $y_1,y_2$ are linearly independent therefore $c_1=\Tilde{c_1}, c_2=\Tilde{c_2}$ and hence the linear combination is \textbf{unique}.

    \item It remains to show that given a solution $y$ we can express it in the form
    \[
        y(t) = c_1y_1(t) + c_2y_2(t)
    \]
    We shall be using the existence and uniqueness theorem to do this.

    Given $c_1y_1(t) + c_2y_2(t)$ is a solution, the unique solution also satisfies the initial conditions. If we have $y(t_0)=b_0, y'(t_0)=b_1$ then
    \[
    \begin{matrix}
        b_0 = c_1y_1(t_0) + c_2y_2(t_0) \\
        b_1 = c_1y_1'(t_0) + c_2y_2'(t_0)
    \end{matrix}\quad\Bigg\}
    \]
    this is a $2\times2$ system of eqns in unknowns $c_1$ and $c_2$
    \[
    \begin{bmatrix}
        y_1(t_0) & y_2(t_0) \\
        y_1'(t_0) & y_2'(t_0)
    \end{bmatrix}
    \begin{bmatrix}
        c_1 \\
        c_2
    \end{bmatrix}
    =
    \begin{bmatrix}
        b_0 \\
        b_1
    \end{bmatrix}
    \]
    Let $A = \begin{bmatrix}
        y_1(t_0) & y_2(t_0) \\
        y_1'(t_0) & y_2'(t_0)
    \end{bmatrix}$ the this system has solutions $c_1,c_2$ as long as $W(y_1,y_2) = \det A = y_1(t_0)y_2'(t_0) - y_1'(t_0)y_2(t_0) \neq 0$. We require $W\neq 0 \forall t$ since $t_0$ was chosen arbitrarily.

    So $c_1y_1(t) + c_2y_2(t)$ satisfies the initial conditions if $W\neq 0$. Since our solution $y$ satisfies the initial conditions too, the existence and uniqueness theorem tells us that $y=c_1y_1(t) + c_2y_2(t) \quad \forall t\in I$ by uniqueness. Since this is true for arbitrary initial conditions we conclude that any solution $y$ can be written as a linear combination of $y_1,y_2$.

    To complete the  we need to show that given 2 linearly independent solutions $y_1,y_2$ of $L[y]$ then $W(y_1(t),y_2(t))\neq 0 \quad \forall t \in I$, But we have already proved this earlier in the corollary in the last section.
\end{enumerate}
\end{proof}

To summarise. Let $y_1, y_2$ be solutions of $L[y] = y'' + p(t)y' + q(t)y$ on $I \subseteq \mathbb{R}$, then the following are equivalent.
\begin{enumerate}[(i)]
    \item $y_1,y_2$ are a fundamental set of solutions,
    \item $y_1,y_2$ are linearly independent on $I$,
    \item $W(y_1,y_2)(t_0)\neq 0 $ for some $t_0 \in I$,
    \item $W(y_1,y_2)(t)\neq 0 $ for all $t \in I$.
\end{enumerate}

\subsection{Reduction of Order}

Reduction of Order provides a way to obtain a second solution to the homogeneous equation $L[y]=0$ if we know one solution.

Let $y_1$ be a solution of $L[y]=y''+p(t)y'+q(t)y=0$. Using Abel's theorem to find the Wronskian $W$ then if $W\neq0$ for some $t$ we can find $y_2$ such that $L[y_2]=0$ and $y_1,y_2$ are linearly independent. Hence we can construct the general solution as follows:

$W(y_1, y_2) = y_1y_2'-y_1'y_2$, so provided $y_1\neq 0$ we can divide by $y_1$
\[
\frac{W}{y_1} = y_2' - \frac{y_1'}{y_1}y_2 \qquad \text{ ie } \quad y_2' - \frac{y_1'}{y_1}y_2 = \frac{W}{y_1}.
\]
This is a first order ODE in $y_2$. We can solve this using the integrating factor
\[
\mu(t) = \exp ( -\int \frac{y_1'}{y_1} dt ) = \exp (-\log(y_1) ) = \frac{1}{y_1},
\]
therefore the equation can be rewritten as 
\[
\left(\frac{y_2}{y_1}\right)' = \frac{W}{y_1}
\]
giving
\[
y_2 = y_1(t)\int \frac{W}{(y_1(t))^2} dt = y_1(t) \int^{t}\frac{1}{y_1^2(x)} \exp \left( -\int^x p(s) ds \right) dt.
\]

\begin{tcolorbox}
    \begin{exmp}\label{exmp:ReductionOfOrder}
        $y_1(t) = \exp(t) $ is a solution to the ODE
        \[
            y'' - 2y' + y = 0.
        \]
        Find the general solution.
    \end{exmp}
    \begin{sol}
        $W = y_1y_2' - y_2y_1'$.
        If $y_2$ is a second solution to the ODE then
        \[
        \frac{dW}{dt} + P(t)W = 0.
        \]
        In this example $P=-2$ therefore $\frac{dW}{dt} = 2W$. We can solve this
        \[
        \int \frac{dW}{w} = 2 \int dt
        \]
        giving $W=Ae^{2t} \neq 0 \text{ if } A \neq 0.$

        Now 
        \begin{align*}
            W(y_1,y_2) &= y_1y_2'-y_2y_1' \\
            &= e^ty_2' - e^ty_2 = Ae^{2t} \\
        \end{align*}    
        Therefore
        \[
            y_2' - y_2 = Ae_t.
        \]
        This is a first order linear eqn which we can solve. The integrating factor is $\mu(t) = e^{-\int 1 dt} = e^{-t}$ so
        \[
            \left(e^{-t}y_2\right)' = A \\
            \implies y_2 = (At+B)e^t
        \]
        Since $Be^t$ is a constant multiple of $y_1$ we define $y_2 = te^t$ (i.e. set $B=0, A=1$) so our general solution is
        \[
            y(t) = c_1e^t + c_2te^t.
        \]
    \end{sol}
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}\label{exmp:RedofOrder2}
        Verify $y_1(t) = t)$ is a solution of the equation
        \[
            (1-t^2)y'' - 2ty' + 2y = 0 \quad t>1
        \]
        and find using the Wronskian, the second linearly independent solution.
    \end{exmp}
    \begin{sol}
        We have $y_1(t) = t, y_1'(t) = 1, y_1''(t) = 0$ so
        \[
            (1-t^2)\cdot 0 -2t \cdot 1 + 2t = 0
        \]
        as required. So $y_1(t)$ is a solution to the ODE.

        Let $y_2(t)$ be a second solution to the ODE and let $W = y_1y_2' - y_2y_1'$. Then
        \begin{align*}
            (1-t^2)W' &= (y_1y_2'' - y_2y_1'')(1-t^2) \\
            &= y_1(1-t^2)y_2'' - y_2(1-t^2)y_1'' \\
            &= y_1(2ty_2' - 2y_2) - y^2(2ty_1' - 2y_1) \\
            &= 2t(y_1y_2' - y_2y_1') \\
            &= 2tW
        \end{align*}
    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}
        Hence $W' = \frac{2t}{1-t^2}W$ i.e.
        \[
        \int \frac{W'}{W} dt = \int \frac{2t}{1-t^2} dt \quad
        \implies \quad W = \exp\left(\int \frac{2t}{1-t^2} dt \right) 
        = \frac{A}{1-t^2} \qquad A \in \mathbb{R}\backslash\{0\}        
        \]
        Since $W = y_1y_2' - y_2y_1'$ and $y_1=t$ we have
        \[
        ty_2'-y_2 = \frac{A}{1-t^2} \quad \text{ i.e. } \quad y_2'-\frac{1}{t}y_2 = \frac{A}{t(1-t^2)}.
        \]
        This is a first order equation which we can solve. It has integrating factor $\mu = e^{\int -\frac{1}{t}dt} = e^{-\ln(t)}=\frac{1}{t}$ so
        \begin{align*}
            \left( \frac{y_2}{t} \right)' &= \frac{A}{t^2(1-t^2)} \\
            y_2 &= t \int \frac{A}{t^2(1-t^2)} dt.
        \end{align*}
        Using partial fractions we get
        \begin{align*}
            y_2 &= At \left[ \int \left( \frac{1}{t^2} + \frac{1}{2(1+t)} + \frac{1}{2(1-t)}\right) dt \right] \\
            &= At \left[ -\frac{1}{t} + \frac{1}{2} \ln \left\vert 1+t \right\vert - \frac{1}{2} \ln \left\vert t-1 \right\vert + b\right] \\
            &= A \left[ -1 + \frac{t}{2}\ln\left( \frac{1+t}{t-1} \right) + Bt \right].\\
        \end{align*}
        where $A,B$ are constants of integration. Now $ABt$ is a multiple of $y_1(t) = t$ so define
        \[
            y_2(t) = -1 + \frac{t}{2}\ln \left( \frac{t+1}{t-1} \right)
        \]
        where we are setting $A=1, B=0$.
    
\end{tcolorbox}

\pagebreak

\section{Power Series Solutions}

In Section \ref{section:LinearDependence&Wronskians} we could find the general solution to the homogeneous ODE
\begin{equation}\label{eq:HomogeneousLinear}
    L[y] = a(x)y'' + b(x)y' + c(x)y = 0
\end{equation}
when we already know one solution to the ODE.

Now we address the issue of how to find the first solution if we can't find one by inspection alone. If $a(x)$ and $b(x)$ and $c(x)$ are polynomials in $x$ then we can use the power series solutions of \eqref{eq:HomogeneousLinear} to find a general solution of the form
\[
    y(t) = \sum_{n=0}^{\infty}a_n (x-x_0)^n = a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + \cdots
\]
Where determining the $a_n$ coefficients is at the heart of the solution technique.

\subsection{Review of Power and Taylor Series}

Taylor series are an example of a power series and they are a generalisation of Maclaurin series.

if we have a function $f(x) = \sum_{n=0}^{\infty} a_n(x-x_0)^n$ then
\begin{align*}
    f(x) &= a_0 + a_1(x-x_0) + a_2(x-x_0)^2 + a_3(x-x_0)^3 + \cdots; &\qquad f(x_0) = a_0 \\
    f'(x) &= a_1 + \cdot2 a_2(x-x_0) + 3 a_3(x-x_0)^2 + \cdots; &\qquad f'(x_0) = a_2 \\
    f''(x) &= 2a_2 + 3\cdot2 a_3(x-x_0) + 4\cdot3 a_4(x-x_0)^2 + \cdots; &\qquad f''(x_0) = 2a_3 \\
    \vdots
\end{align*}
So the Taylor series expansion of $f(x)$ about some point $x_0$ is $$f(x)=f(x_0) + f'(x_0)(x-x_0) + \frac{f''(x_0)}{2!} (x-x_0)^2 + \cdots$$
In order to talk about power series solutions we need the series to converge.

We have absolute convergence if
\[
\lim_{N\to \infty} \sum_{n=0}^{N} \left\vert a_n(x-x_0)^n \right\vert \quad \text{converges.}
\]
We can check for convergence using the ratio test:
\[
\lim_{N\to \infty} \left\vert \frac{a_{n+1} (x-x_0)^{n+1}}{a_n(x-x_0)^n} \right\vert = \vert x-x_0 \vert \lim_{N\to \infty} \left\vert \frac{a_{n+1}}{a_n} \right\vert = L
\]
If
\[
\begin{cases}
    L < 1 & \text{ the series converges for that $x$} \\
    L = 1 & \text{ convergence/divergence cannot be determined} \\
    L > 1 & \text{ the series diverges for that $x$} \\
\end{cases}
\]
The radius of convergence is all $x$ for which $L<1$. So if $L<1$ for some $\vert x - x_0 \vert < p$ this gives the radius of convergence $p$.

\begin{tcolorbox}
    \begin{exmp}\label{exmp:PowerSeriesConvergence}
        Does $S = \sum_{n=1}^{\infty} \frac{(-1)^2 x^{2n}}{(2n)!}$, near $x=0$ converge absolutely?
    \end{exmp}
    \begin{sol}
        \[
        L = \lim_{N\to \infty} \left\vert \frac{(-1)^{n+1}\  x^{2n+2}\ (2n)!}{(-1)^n\ x^{2n}\ (2n+2)!} \right\vert 
        = \vert x \vert^2 \lim_{N \to \infty} \left\vert \frac{1}{(2n+1)(2n+2)} \right\vert = 0 < 1.
        \]
        So $S$ converges absolutely. In fact $S$ is the series expansion for $\cos(x)$.
    \end{sol}
\end{tcolorbox}

\subsection{Power Series Solution Techniques}

Consider the differential equation
\[
a(x)y'' + b(x)y' + c(x)y = 0
\]
where $a(x),b(x),c(x)$ are polynomials. We look for power series solutions near the point $x_0$. The power series solution is valid if the series converges.

\subsubsection{Ordinary Points}

\begin{definition}
    A point $x_0 \in \mathbb{R}$ is called an \textit{ordinary point} of equation \eqref{eq:HomogeneousLinear} if $a(x_0) \neq 0$ and otherwise $x_0$ is called a \textit{singular point}.
\end{definition}

\begin{tcolorbox}
    \begin{exmp}
        Solve $y'' -xy' - y = 0$ about $x_0=0$.
    \end{exmp}
    \begin{sol}
        We first note that every point is an ordinary point since $a(x)=1$. Next we seek a power series solution $y=\sum_{n=0}^{\infty} a_n(x-x_0)^n$, where $x_0 = 0$.
        Hence
        \[
        y' = \sum_{n=0}^{\infty} na_n(x-x_0)^{n-1}, y''= \sum_{n=0}^{\infty} n(n-1)a_n(x-x_0)^{n-2}.
        \]
        Substituting into the ODE gives:
        \[
        \sum_{n=0}^{\infty} n(n-1)a_nx^{n-2} - x \sum_{n=0}^{\infty} na_nx^{n-1} - \sum_{n=0}^{\infty} a_nx^n = 0.
        \]
        Combining the sums
        \[
            \sum_{n=0}^{\infty}  \left[ n(n-1)a_nx^{n-2} - na_nx^n - a_nx^n \right] = 0
        \]
        $x^0, x^1, x^2, \ldots, x^n$ are linearly independent so the sum is zero provided the coefficients of each power of $x$ are zero.

        The coefficient of $x^N$ is:
        \[
        (N+2)(N+1)a_{N+2} - Na_N - a_N = 0
        \]
        Rearranging:
        \begin{equation}
            \tag{Recurrence relation}
            a_{N+2} = a_N \left[ \frac{1}{N+2} \right]
        \end{equation}
        Given $a_0$ we can use the recurrence relation to determine the coefficients of all the even powers of $x$. Likewise we can determine all the odd powers given $a_1$. So
        \[
        a_2 = \frac{a_0}{2}, \quad a_4=\frac{a_2}{4}=\frac{a_0}{2\cdot 4}, \quad a_6=\frac{a_4}{6}=\frac{a_0}{2\cdot4\cdot6}, \quad\cdots
        \]
        and
        \[
        a_3 = \frac{a_1}{3}, \quad a_5=\frac{a_3}{5}=\frac{a_1}{3\cdot 5}, \quad a_7=\frac{a_5}{7}=\frac{a_1}{3\cdot5\cdot7}, \quad\cdots
        \]
        We can group the even and odd terms to get
        \[
         y = a_0\left(1+\frac{x^2}{2} + \frac{x^4}{2\cdot4} + \frac{x^6}{2\cdot4\cdot6} + \cdots\right) + a_1\left(1+\frac{x^3}{3} + \frac{x^5}{3\cdot5} + \frac{x^7}{3\cdot5\cdot7} + \cdots\right)
        \]
        or simplifying:
        \[
            y = a_0 \sum_{n=0}^{\infty} \frac{x^{2n}}{2^nn!} + a_1\sum_{n=0}^{\infty}\frac{2^nn!x^{2n+1}}{(2n+1)!}
        \]
    \end{sol}
    
\end{tcolorbox}
\begin{tcolorbox}
        We expect two linearly independent solutions to the ODE with two arbitrary coefficients $c_1, c_2$ to give the general solution $y=c_1y_1 + c_2y_2$. This is what we have above with $a_0=c_1$ and $a_1=c_2$ and the two solutions $y_1,y_2$ are given by the two power series.

        Suppose we are given initial conditions $y(0)=A$ and $y'(0)=B$, we can readily see that $a_0=A$ and $a_1=B$. So the unique solution to the IVP is then 
        \[
        y = A \sum_{n=0}^{\infty} \frac{x^{2n}}{2^nn!} + B\sum_{n=0}^{\infty}\frac{2^nn!x^{2n+1}}{(2n+1)!}
        \]
        We use the ratio test to check the convergence of each power series. For the first series:
        \[
        \lim_{n\to \infty} \left\vert \frac{\frac{x^{2(n+1)}}{2^{n+1}(n+1)!}}{\frac{x^{2n}}{2^nn!}} \right\vert = \lim_{n\to \infty} \left \vert \frac{1}{2(n+1)} \right \vert x^2 = 0.
        \]
    
        So $L=0$ and the series converges for all $x$. For the second series
        \[
        \lim_{n\to \infty} \left\vert \frac{\frac{x^{2(n+1)+1} \ 2^{n+1}(n+1)!}{(2(n+1)+1)!}}{\frac{x^{2n+1}\ 2^nn!}{(2n+1)!}} \right\vert = \lim_{n\to \infty} \left \vert \frac{1}{2n+3} \right \vert x^2 = 0.
        \]
        So $L=0$ and the series converges for all $x$.
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}
        Find the recurrence relation corresponding to the power series solution of $y''+xy=0$ around the point $x_0=2$.
    \end{exmp}
    \begin{sol}
        Since $a(x)=1$ every point is an ordinary point. Looking for a power series solution of the form $y=\sum_{n=0}^{\infty} a_n(x-2)^n$ gives
        \[
        y''= \sum_{n=0}^{\infty} n(n-1)a_n(x-2)^{n-2}.
        \]
        Substituting these expressions into the ODE yields:
        \[
            \sum_{n=0}^{\infty} \left[ n(n-1)a_n(x-2)^{n-2} + xa_n(x-2)^n\right] = 0,
        \]
        rewriting $x=x-2+2$ we have
        \[
            \sum_{n=0}^{\infty} \left[ n(n-1)a_n(x-2)^{n-2} + a_n(x-2)^{n+2} + 2a_n(x-2)^n\right] = 0.
        \]
        The coefficient of $(x-2)^N$ is:
        \[
        (N+2)(N+1)a_{N+2} + a_{N-1} + 2a_N = 0,
        \]
        so the recurrence relation for the coefficients is
        \[
        \begin{cases}
            a_{N+2} = \frac{-2a_N - a_{N-1}}{(N+2)(N+1)} & \text{for } N \geq 1 \\
            a_2 = a_0 & \text{for } N=0.
        \end{cases}
        \]
    \end{sol}
\end{tcolorbox}

\subsubsection{Regular Singular Points}

Returning to equation \eqref{eq:HomogeneousLinear}:
\[
a(x)y'' + b(x)y' + c(x)y = 0
\]
on dividing by $a(x)$ we get
\begin{equation}\label{eq:dividedHomogeneous}
    y'' + p(x)y' + q(x)y = 0, \quad p(x) = \frac{b(x)}{a(x)}, \ q(x)=\frac{c(x)}{a(x)}.
\end{equation}
If we can write the solution as a Taylor series
\[
    y(x) = \sum_{n=0}^{\infty} a_n(x-x_0)^n
\]
then we have seen $a_0=y(x_0),\ a_1=y'(x_0),\ a_2=\frac{y''(x_0)}{2},\ \ldots$

We are given $y(x_0)$ and $y'(x_0)$ in an IVP so from \eqref{eq:HomogeneousLinear} we have
\[
    y''(x_0) = -p(x_0)y'(x_0) - q(x_0)y(x_0)
\]
We can calculate higher derivatives of $y$ and hence find $a_n$ by repeatedly differentiating the ODE \eqref{eq:HomogeneousLinear}. This works provided $p(x), q(x)$ are infinitely differentiable at $x_0$. So we need $p(x), q(x)$ to be analytic at $x_0$, that is they need to have a convergent Taylor series expansion in some interval about $x_0$:
\begin{align*}
    p(x) &= p_0 + p_1(x-x_0) + p_2(x-x_0)^2 + \cdots = \sum_{k=0}^{\infty} p_k(x-x_0)^k, \\
    q(x) &= q_0 + q_1(x-x_0) + q_2(x-x_0)^2 + \cdots = 
    \sum_{k=0}^{\infty} q_k(x-x_0)^k. \\
\end{align*}
If $x_0$ is an ordinary point then this is not a problem. To solve \eqref{eq:HomogeneousLinear} in the neighbourhood of a singular point $x_0$ we have to restrict ourselves to the case where the singularity in $\frac{b(x)}{a(x)}$ and $\frac{c(x)}{a(x)}$ is not too severe. This means the singularity of $\frac{b}{a}$ is no worse than $(x-x_0)^{-1}$ and the singularity of $\frac{c}{a}$ is no worse than $(x-x_0)^{-2}$. Such a point is a regular singular point.

\begin{definition}
    Given a singular point $x_0$, it is a \textit{regular singular point} of equation \eqref{eq:HomogeneousLinear} if
    \[
    \lim_{x\to x_0}(x-x_0)\frac{b(x)}{a(x)} \quad \text{and} \quad \lim_{x\to x_0}(x-x_0)^2\frac{c(x)}{a(x)}
    \]
    are both finite. If these conditions do not hold, then the singular point is \textit{irregular}.
\end{definition}

So we want $(x-x_0)p(x)$ and $(x-x_0)q(x)$ to be analytic about $x_0$, that is they have a convergent power series of the form
\[
(x-x_0)p(x) = \sum_{k=0}^{\infty}p_k(x-x_0)^k, \qquad (x-x_0)^2q(x) = \sum_{k=0}^{\infty}q_k(x-x_0)^k
\]
on some $\vert x- x_0 \vert < \rho, \ \rho>0$.

Multiplying \eqref{eq:dividedHomogeneous} by $(x-x_0)^2$ gives:
\[
(x-x_0)^2y'' + (x-x_0)^2p(x)y' + (x-x_0)^2q(x)y = 0
\]
i.e.
\[
(x-x_0)^2y'' + (x-x_0)\left[\sum_{k=0}^{\infty}p_k(x-x_0)^k \right]y' + \left[ \sum_{k=0}^{\infty}q_k(x-x_0)^k\right]y = 0
\]
If all of $p_k, q_k$ are zero except $p_0, q_0$ then we have \textbf{Euler's equation}.
\begin{equation}\label{eq:EulersEquation}
    \colorboxed{orange}{(x-x_0)^2y'' + (x-x_0)p_0y' + q_0y = 0}
\end{equation}

\subsubsection{Euler's equation}

It is easy to see Euler's equation has a singular point at $x_0$, moreover 
\[
\lim_{x\to x_0}(x-x_0)\frac{p_0}{(x-x_0)} = p_0 \quad \text{and} \quad \lim_{x\to x_0}(x-x_0)^2\frac{q_0}{(x-x_0)^2} = q_0
\]
are both finite so $x_0$ is a regular singular point.

Solving Euler's equation is done by using $y=(x-x_0)^r$ so $y' = r(x-x_0)^{r-1}, y''=r(r-1)(x-x_0)^{r-2},$ substituting these into \eqref{eq:EulersEquation} gives
\[
(x-x_0)^2\left(r(r-1)(x-x_0)^{r-2}\right) + p_0(x-x_0)r(x-x_0)^{r-1} + q(x-x_0)^r = 0
\]
i.e. $(x-x_0)^r\Big[ (r-1)r + p_0r + q_0 \Big] = 0$.
This has solutions provided
\[
r^2 + r(p_0 - 1) + q_0 = 0
\]
i.e.
\[
r_{\pm} = \frac{1}{2} \left[-(p_0-1) \pm \sqrt{(p_0-1)^2 - 4q_0} \right]
\]
\begin{itemize}
    \item if $(p_0-1)^2-4q_0 > 0$ then $r_\pm$ are \textbf{distinct real roots} and
    \[
    y(x) = c_1(x-x_0)^{r_+}+c_2(x-x_0)^{r_-}, \quad c_1,c_2 \in \mathbb{R}.
    \]
    \item if $(p_0-1)^2-4q_0 < 0$ then $r_{\pm}$ are \textbf{complex}. Let $r_{\pm} = \lambda \pm i\mu$ then 
    \begin{align*}
        (x-x_0)^{\lambda\pm i\mu} &= (x-x_0)^\lambda(x-x_0)^{\pm i\mu}\\
        &= (x-x_0)^\lambda e^{\ln (x-x_0)^{\pm i\mu}} \\
        &= (x-x_0)^\lambda e^{\pm i\mu \ln(x-x_0)}.
    \end{align*}
    Thus
    \[
    y(x) = c_1(x-x_0)^\lambda\cos\left(\mu\ln\vert x-x_0\vert\right) + c_2(x-x_0)^\lambda\sin\left(\mu\ln \vert x-x_0 \vert\right).
    \]
    \item if $(p_0-1)^2-4q_0 = 0$ then $r_+=r_-$ and we have a \textbf{double root}.
    Then $y_1=(x-x_0)^{r_+}$ and we use the Wronskian approach from Chapter 1 to find the second linearly independent solution.
\end{itemize}

\begin{tcolorbox}
    \begin{exmp}
        Solve $x^2y''-2y=0$.
    \end{exmp}
    \begin{sol}
        The ODE is an Euler equation with $x_0=0$. We look for solutions of the form $y=x^r$; thus
        \[
        r(r-1) - 2=0 \ \implies \ (r-2)(r+1)=0.
        \]
        Hence $r_+=2, r_-=-1$. The general solution is therefore
        \[
        y(x) = c_1x^2 + c_2x^{-1}, \quad c_1,c_2 \in \mathbb{R}.
        \]
        Notice the solution blows up at $x=0$ as expected as $x=0$ is a singular point.
    \end{sol}
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}
        Solve $x^2y'' + 5xy' + 4y = 0$.
    \end{exmp}
    \begin{sol}
        The ODE is an Euler equation with $x_0=0$. Looking for solutions of the form $y=x^r$ yields
        \[
        r(r-1) + 5r + 4 = (r+2)^2 = 0
        \]
        so $r_\pm=-2$ (repeated root).

        We have found one solution $y1=x^{-2}$, we can use the Wronskian to find the second linearly independent solution $y_2$:
        \begin{align*}
        W' &= y_1y_2'' - y_2y_1'' = y_1\left(\frac{-5}{x}y_2'-\frac{4}{x^2}y_2\right) - y_2\left(\frac{-5}{x}y_1'-\frac{4}{x^2}y_1 \right)\\
        &= \frac{5}{x}(y_1y_2' - y_2y_1') = \frac{-5}{x}W.
        \end{align*}
        Solving this ODE in $W$:
        \[
        W = A \exp\left(-\int \frac{5}{x} dx \right) = A x^{-5}, \quad A \in \mathbb{R}.
        \]
        Using $W=y_1y_2' - y_1'y_2 = x^{-2}y_2' + 2x^{-3}y_2 = Ax^{-5}$, hence
        \[
        y_2' + 2x^{-1}y_2 = Ax^{-1}.
        \]
    \end{sol}
    
        
        Using the integrating factor $\mu(x) = \exp\left(\int2x^{-1} dx \right) = x^2$ we find
        \[
        y_2(x) = \frac{1}{x^2}\int \frac{A}{x} dx = \frac{A}{x^2}\ln x + Bx^{-2}
        \]
        where $A,B$ are constants of integration. Since $Bx^{-2}$ is a constant multiple of $y_1$, we can take $B=0$. The general solution is therefore
        \[
        y(x) = c_1y_1 + c_2y_2 = c_1x^{-2} + c_2x^{-2}\ln x.
        \]
        Notice the solution is singular at $x=0$ as expected.
\end{tcolorbox}

\subsubsection{Method of Frobenius}

Returning back to equation \eqref{eq:dividedHomogeneous} we had
\[
(x-x_0)^2y'' + (x-x_0)\left[ (x-x_0)p(x)\right]y' + \left[ (x-x_0)^2q(x)\right]y = 0
\]
If $p,q$ are analytic around $x_0$ then there exists convergent power series such that
\[
(x-x_0)^2y'' + (x-x_0)\left[\sum_{k=0}^{\infty}p_k(x-x_0)^k \right]y' + \left[ \sum_{k=0}^{\infty}q_k(x-x_0)^k\right]y = 0
\]
i.e.
\[
\underbrace{(x-x_0)^2y'' + (x-x_0)p_0y' + q_0y}_{\text{LHS of Euler's Equation}} + \underbrace{(x-x_0)\Big\{ p_1(x-x_0)y' + q_1y + p_2(x-x_0)^2y' + q_2(x-x_0)y + \cdots \Big\}}_{\text{higher order terms}} = 0
\]
The higher order terms cannot introduce more singular terms but rather corrections that are higher powers of $(x-x_0)$ so we look for solutions of the form
\[
y(x) = (x-x_0)^r\sum_{k=0}^{\infty} a_n(x-x_0)^n = \sum_{k=0}^{\infty} a_n(x-x_0)^{n+r}
\]
which is known as a Frobenius series.

\textbf{Method of Frobenius:}

To determine a solution of \eqref{eq:dividedHomogeneous} we need to know
\begin{enumerate}[1)]
    \item the values of $r$,
    \item a recurrence relation for the $a_n$'s,
    \item determine the radius of convergence of the resulting power series.
\end{enumerate}

\begin{tcolorbox}
    \begin{exmp}
        Solve $2x^2y'' - xy' + (1+x)y = 0$.
    \end{exmp}
    \begin{sol}
        $a(x)=2x^2$ hence $x=0$ is a singular point.
        \[\lim_{x\to0} x\frac{b(x)}{a(x)} = \lim_{x\to0} \frac{x(-x)}{2x^2} = \frac{-1}{2} \ \text{ and } \ \lim_{x\to0} x^2 \frac{c(x)}{a(x)} = \lim_{x\to0} x^2 \frac{(1+x)}{2x^2} = \frac{1}{2}.\]
        Thus $x=0$ is a regular singular point.
        We look for a Frobenius series solution $y=\sum_{n=0}^{\infty} a_nx^{n+r} $, where $a_0 \neq 0$.

        We have
        \[
            y' = \sum_{n=0}^{\infty} a_n(n+r)x^{n+r-1} \quad \text{ and } \quad y''=\sum_{n=0}^{\infty} a_n(n+r)(n+r-1)x^{n+r-2}.
        \]
        Substituting these into the ODE gives:
        \[
            \sum_{n=0}{\infty} \left[ 2x^2a_n(n+r)(n+r-1)x^{n+r-2} -xa_n(n+r)x^{n+r-1} + (1+x)a_nx^{n+r} \right] = 0.  
        \]
        To find an equaiont defining $r$ we consider the coefficient of the lowest power of $x$ ($x^r$) which gives:
        \[a_02r(r-1)-a_0r+a_0 = 0, \qquad \text{where } a_0\neq0\].
        \[\implies (2r-1)(r-1) = 0 \qquad \colorboxed{orange}{\text{Indicial Equation}}\]
        So $r=\frac{1}{2}$ or $r=1$.
        The indicial equation is responsible for capturing the correct behaviour of the singularity.
        The coefficient of a general power of $x$ ($x^{N+r}$) is:
        \[
            a_N2(N+r)(N+r-1)-a_N(N+r)+a_N+a_{N-1} = 0
        \]
        giving
        \[
            a_N = \frac{-a_{N-1}}{(N+r-1)(2(N+r)-1)} \quad \colorboxed{orange}{\text{Recurrence Relation.}}
        \]
        This allows us to determine each of the coefficients in the power series expansion for a given value of $r$.

        \textbf{Case 1}($r=1$). Recurrence relation: $a_n = \frac{-a_{n-1}}{n(2n+1)}.$

        The corresponding power series solution which is valid for $x>0$ is:
        \[y_1(x) = x\left[ \sum_{n=0}^{\infty} \frac{(-1)^n\ x^n\ 2^n \ n!}{(2n+1)! \ n!} \right]\]

        Note: we arbitrarily took $a_0=1$. Recall that the general solution is $y=c_1y_1 + c_2y_2$ where $c_1,c_2$ account for the arbitrary value of $a_0$.

    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}

        \textbf{Case 2}($r=\frac{1}{2}$). Recurrence relation: $a_n=\frac{-a_{n-1}}{(2n-1)n}$.

        The corresponding power series solution which is valid for $x>0$ is:
        \[ y_2 = x^{\frac{1}{2}} \left[ \sum_{n=0}^{\infty} \frac{(-1)^n \ 2^n \ x^n}{(2n)!} \right] \]

        To determine the exact range of validity of the series we perform the ratio test.
        
        \textbf{Series $y_1$}:
        \[\lim_{n\to \infty} \left \vert \frac{x}{(n+1)(2n+3)} \right \vert = 0\]
        hence $y_1$ is valid for all $x>0$, as the series converges absolutely.

        \textbf{Series $y_2$}:
        \[\lim_{n\to \infty} \left \vert \frac{-x2x}{(2n)(2n+1)(n+1)} \right \vert = \lim_{n\to \infty} \left \vert \frac{x}{(2n+1)(n+1)} \right \vert = 0 \]
        hence $y_2$ converges absolutely for $x>0$.
\end{tcolorbox}

In the above example the indicial equation had real distinct roots. If the roots are complex then we would proceed in the same way, but we would find complex-valued functions of $x$ for $y_1$ and $y_2$, however as with Euler's equation we can obtain real-values solutions by taking real and imaginary parts of the complex solutions.

\textbf{Repeated Real Roots:}

If the indicial equation has a repeated root $r_1=r_2$ then $(r-r_1)^2=0$. We need to determine $a_n$ as a function of $r$ and we arrive at one solution
\[
y_1(x) = \sum_{n=0}^{\infty} a_n(r_1)(x-x_0)^{n+r_1}.
\]
The second solution is
\[
y_2(x) = y_1(x)\ln (x-x_0) + \sum_{n=1}^{\infty} \frac{\partial a_n}{\partial r}\Big\vert_{r=r_1} (x-x_0)^{n+r_1}.
\]
We illustrate how this second solution is found via the following example.

\begin{tcolorbox}
    \begin{exmp}[Bessel's equation of order 0]
        Find a series solutions about $x=0$ for the ODE:
        \[
        L[y] = x^2y'' + xy' + x^2y = 0.
        \]
    \end{exmp}
    \begin{sol}
        $x=0$ is a singular point and
        \[
          \lim_{x\to0} x\frac{b(x)}{a(x)} = \lim_{x\to0}\frac{xx}{x^2}=1, \text{ and } \lim_{x\to0} \frac{x^2 c(x)}{a(x)} = \lim_{x\to0} \frac{x^2x^2}{x^2} = 0.  
        \]
        As both limits are finite $x=0$ is a regular singular point. Looking for series solutions of the form $y = \sum_{n=0}^{\infty} a_n x^{n+r}, \ a_0 \neq 0$ gives
        \begin{equation}\label{eq:SeriesFrobeniusSum}
            \sum_{n=0}^{\infty} \left[ a_n(n+r)(n+r-1)x^{n+r} + a_n(n+r)x^{n+r} + a_nx^{n+r+2} \right] = 0.
        \end{equation}
        Coefficients of $x^r$ are:
        \[ r(r-1)+r = r^2 = 0 \quad \colorboxed{orange}{\text{Indicial Equation}}\]
    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}
        Hence $r=0$ is a repeated root. The coefficients of $x^{r+1}$ are:
        \[a_1(r+1)r + (r+1)a_1 = a_1(r+1)^2 = 0,\]
        since $r=0$ we must have $a_1=0$.
        The coefficients of a general term $x^{N+r}$ are:
        \[ a_N(N+r)(N+r-1) + a_N(N+r) + a_{N-2} = 0, \quad N\geq 2.\]
        Hence the \colorboxed{orange}{\text{recurrence relation}} is
        \[ a_N = \frac{-a_{N-2}}{(r+N)^2}, \quad N\geq2 \]
        
    

        \textbf{First solution $y_1$:} Setting $r=0$ in the recurrence relation gives $a_N = \frac{-a_{N-2}}{N^2}.$ Since $a_1=0$ it follows that $a_3,a_5,a_7,\ldots$ are all zero. The only $a_N$ which are non-zero correspond to even $N$. Let $N=2m$, $m\in \mathbb{N}$ and reformulate the reccurence relation:
        \[ a_{2m} = \frac{-a_{2(m-1)}}{(2m)^2}. \]
        So
        \[ a_2 = \frac{-a_0}{(2\cdot1)^2}, \quad a_4 = \frac{-a_2}{(2\cdot2)^2}= \frac{+a_0}{2^2 2^2 (1\cdot2)^2}, \quad a_6 = \frac{-a_4}{(2\cdot3)^2} = \frac{-a_0}{2^2 2^2 2^2 (1\cdot2\cdot3)^2}, \cdots  \]
        and
        \[ a_{2m} = \frac{(-1)^m\ a_0}{2^{2m}\ (m!)^2} \]
        Hence
        \[ y_1(x) = \sum_{m=0}^{\infty} \frac{(-1)^m\ x^{2m}}{2^{2m}\ (m!)^2}\].

        This is the Bessel function of the first kind of order zero and is denoted $J_0(x)$.

        \textbf{Ex.} Check the series converges absolutely.

        \textbf{Second solutions $y_2$:} As we are dealing with power series it would be difficult to use the Wronskian to find the second linearly independent solution. We use and alternative approach.

        Rewriting equation \eqref{eq:SeriesFrobeniusSum}, $L[y]$ gives 
        \[
            L[y] = a_0r^2x^r+
            \underbrace{a_1(r+1)^2x^{r+1}}_{a_1=0}
             + \sum_{n=2}^{\infty}
                 \underbrace{\Big[ a_n(n+r)^2 + a_{n-2}\Big]}_{=0 \text{ by the reccurence relation}} 
                 x^{n+r}
        \]
        Hence $L[y] = a_0r^2x^r $.
        Differentiating with respect to $r$ gives
        \[
            L\left[\frac{\partial y}{\partial r}\right] = 2a_0rx^r + a_0r^2x^r\ln x
        \]
        Observer $L\left[ \frac{\partial y}{\partial r} \Big \vert_{r=0} \right] = 0$ so $\frac{\partial y}{\partial r} \Big \vert_{r=0}$ is a solution to the ODE $L[y]=0$. This is the second solution we were looking for.
        \[ y_2(x) = \frac{\partial y}{\partial r} \Big \vert_{r=0} = \frac{\partial }{\partial r} \left[ \sum_{n=0}^{\infty} a_n(r)x^{n+r} \right] \Bigg \vert_{r=0} \]

        Using the product rule
        \[ \frac{\partial y}{\partial r} \Big \vert_{r=0} = \frac{\partial }{\partial r} \left[ x^r \ln x \left( \sum_{n=0}^{\infty} a_n(r)x^n \right)+ x^r \sum_{n=1}^{\infty} \frac{\partial a_n(r)}{\partial r}x^{n} \right] \Bigg \vert_{r=0} \]
        Hence
        \[
            y_2(x) = \ln x y_1(x) + \left[ \sum_{n=1}^{\infty} \frac{\partial a_n(r)}{\partial r}\Big \vert_{r=0} x^{n} \right].
        \]
        
    \end{tcolorbox}
    \begin{tcolorbox}
        We now need to find $ \frac{\partial a_n(r)}{\partial r}\Big \vert_{r=0}$ using the recurrence relation
        \[
            a_N = \frac{-a_{N-2}}{(r+N)^2}.
        \]
        Hence,
        \[ a_2=\frac{-a_0}{(r+2)^2} \implies \frac{\partial a_2}{\partial r} = \frac{2a_0}{(r+2)^3} \text{ and } \frac{\partial a_2}{\partial r} \Big \vert_{r=0} = \frac{a_0}{4}.
        \]
        \[
            a_4 = \frac{-a_2}{(r+4)^2} = \frac{a_0}{\big[ (r+4)(r+2)\big]^2} \implies \frac{\partial a_4}{\partial r} = \frac{-2a_0(2r+6)}{\big[(r+4)(r+2)\big]^3} \text{ and } \frac{\partial a_4}{\partial r} \Big \vert_{r=0} = \frac{-3a_0}{128}.
        \]

    
        Hence the first few terms of $y_2$ are:
        \[ y_2(x) = \ln(x)y_1(x) + \left[ \frac{1}{4}x^2 - \frac{3}{128}x^4 + \cdots \right], \quad x>0 \]

        Note $y_1(x)\to1$ as $x\to 0$ so $y_2(x)$ has logarithmic singularity at $x=0$. The gernal solution is $y=c_1y_1 + c_2y_2, \ c_1,c_2 \in \mathbb{R}$.
        

\end{tcolorbox}

\pagebreak

\section{Inhomogeneous Equations}

\begin{thm}
Every solution $y$ of the inhomogeneous equation
\[ L[y] = f \]
with $L[y] = y'' + py' + qy$, where $p,q$ and $f$ are continuous ffunctions, is given by
\[ y = c_1y_1 + c_2y_2 + y_p \]
where the function $y_1$ and $y_2$ are fundamental solutions of the homogeneous equation $L[y]=0$ and $y_p$ is any solution of the inhomogeneous equation $L[y] = f$. 
\end{thm}

\begin{proof}
    Let $y$ be any solution of $L[y]=f$. We already have $y_p$ such that $L[y_p] = f$, so $L[y]-L[y_p] = f-f = 0$ hence $L[y-y_p] = 0$ by linearity of $L$.

    Hence $y-y_p$ is a solution of the homogenenous equation, but all solution to the homogenenous equation can be written as a linear combination of a pair of fundamental solutions $y_1,y_2$. So there exists $c_1,c_2 \in \mathbb{R}$ such that $y-y_p=c_1y_1 + c_2y_2$.

    Since for every $y$, a solution of $L[y]=f$ we can find a $c_1,c_2$ we have $y=c_1y_1 + c_2y_2 + y_p$ for every solution.
\end{proof}

\subsection{Exact Second Order Equations}

The general second order ODE
\begin{equation}\label{eq:generalLinear}
    a(x)y'' + b(x)y' + c(x)y = f(x)
\end{equation}
can be writen as
\begin{equation}\label{eq:exactRewrite}
    \big( a(x)y'' - a'(x)y\big)' + \big(b(x)y\big)' + \big(a''(x) - b'(x) + c(x)\big)y = f(x)
\end{equation}
We check by exapanding (3.2):
\[ a'y'+ay'' - a''y + by' + b'y + a''y - b'y + cy = f  \implies ay'' + by' + cy = f \text{ as required.}\]

The differential equation is exact is
\begin{equation}\label{eq:exactSuffCrit}
    a''-b'+c = 0
\end{equation}
When \eqref{eq:exactRewrite} is exact we can integrate to obtain
\[ a(x)y' - a'(x)y + b(x) y = \int f(x) dx + c_1 \]
where $c_1 \in \mathbb{R}$. This is a first order ODE that we can solve using integrating factors.

\begin{tcolorbox}
    \begin{exmp}
        Find the general solution to
        \[
            \frac{1}{x}y'' + \left( \frac{1}{x} - \frac{2}{x^2}\right)y' - \left(\frac{1}{x^2} - \frac{2}{x^3}\right)y = e^x.
        \]
    \end{exmp}
    \begin{sol}
        \[a(x) = \frac{1}{x}, \quad b(x) = \frac{1}{x} - \frac{2}{x^2},\quad c(x) = \frac{-1}{x^2} + \frac{2}{x^3},\quad f(x)=e^x \]
        So \eqref{eq:exactSuffCrit} gives: $2x^{-3} + x^{-2} - 4x^{-3} - x^{-2} + 2x^{-3} = 0$ and hence the ODE is exact. Writing the ODE in the form of \eqref{eq:exactRewrite} gives:
        \[ \left(x^{-1}y' + x^{-2}y\right)' + \left(\left(x^{-1}-2x^{-2}\right)y\right)' = e^x. \]
        Integrating with respect to $x$ gives:
        \[ x^{-1}y' + x^{-2}y + (x^{-1}-2x^{-2})y = e^x + c_1, \quad c_1 \in \mathbb{R} \]
        Hence,
        \[ y' + (1-x^{-1})y = xe^x + c_1x. \]
        Solving the ODE using the integrating factor $\mu(x) = \exp\left( \int 1-\frac{1}{x} dx \right) $ gives the general solution
        \[ y(x) = \frac{1}{2}xe^x + c_1x + c_2xe^{-x}, \]
        $c_1,c_2$ are arbitrary constants.
    \end{sol}
\end{tcolorbox}

\subsection{The Adjoint Equation and Integrating Factors}

If the differential equation \eqref{eq:generalLinear} can be multiplied by a function $z(x)$ so that the resulting ODE is exact then $z(x)$ is called an integrating factor for equation \eqref{eq:generalLinear}.

$z(x)$ times equation \eqref{eq:generalLinear} gives:
\begin{equation} 
    zay'' + zby' + zcy = zf.
\end{equation}
This equation is exact if the LHS can be written as
\begin{equation} \label{eq:zMultCrit}
    zay'' + zby'' + zay = \frac{d}{dx}\left(U(x)y' + V(x)y\right)
\end{equation}
If such a $z(x)$ exists then equation \eqref{eq:zMultCrit} becomes
\begin{equation}\label{eq:exactSol}
    \frac{d}{dx}\left( U(x)y' + V(x)y\right) = z(x)f(x)
\end{equation}
which can be integrated to obtain
\[
    U(x)y' + V(x)y = \int z(x)f(x) dx
\]
This is a first order linear ODE in $y$ which we can sovle. So it remains to find $z(x),U(x)$ and $V(x)$. From equation \eqref{eq:zMultCrit} we have
\begin{align*}
    zay'' + zby' + zcy &= \frac{d}{dx}\left( U(x)y' + V(x)y \right) \\
    &= Uy'' + (U'+V)y' + V'y \\
\end{align*}
Comparing coefficients of $y'', y'$ and $y$ we obtain
\[
\begin{matrix}
    za = U & \quad \text{(i)} \\
    zb = U'+V & \quad \text{(ii)} \\
    zc = V' & \quad \text{(iii)} \\
\end{matrix}    
\]
The derivative of (ii) gives
\[U'' + v@ = (zb)' \quad \text{(iv)}\]
From equations (i) and (iii), we have
\[ U''+V' = (za)'' + zc \quad \text{(v)} \]
Hence
\begin{equation}\label{eq:adjointOfOperator}
    \bar{L}[z] = \frac{d^2}{dx^2}(za) - \frac{d}{dx} (zb) + zc
\end{equation}
is the \colorboxed{orange}{\text{adjoint}} of $L[y] = ay''+by' + cy$.

The adjoint equation is easier to solve. Once $z(x)$ is found we have expressions for $U$ and $V$ and we can solve the exact differential equation \eqref{eq:exactSol}.

\begin{tcolorbox}
    \begin{exmp}
        Find the general solution of
        \[ L[y]= y'' + 4y = x^2 \]
    \end{exmp}
    \begin{sol}
        We multiply the differential equation by $z(x)$ yielding
        \[ zy'' + z4y = zx^2. \]
        Reexpressing the LHS as
        \begin{align*}
            zy'' + z4y &= (Uy' + Vy)' \\
            &= Uy'' + (U'+V)y' + V'y \\
        \end{align*}
        and equating coefficients of $y'',y',y$ gives:
        \[ \begin{matrix}
            z=U & \quad (i) \\
            0 = U + V & \quad (ii) \\
            4z = V' & \quad (iii) \\
        \end{matrix}
        \]
        From (ii): 
        \[U''+V'=0 \quad \text{(iv)} \]
        From (i) and (iii):
        \[ U''+V' = z'' + 4z \quad \text{(v)} \]
        Then (v) minus (iv) gives:
        \[ z'' + 4z = 0 \]
        The adjoint of $L$ is therefore:
        \[ \bar{L} = \frac{d^2}{dx^2} + 4. \]

        Solving $\bar{L}[z] = 0$ to find $z(x)$ can be done by looking for solutions of the form $z(x) = e^{mx}$ which gives the auxiliary equation
        \[ m^2 + 4 = 0. \]
        Hence $m = \pm 2i$. Since we only need one solution for $z(x)$ and not the general solution it sufficies to take $z(x)=\sin(2x)$.
        
    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}
    

        From here, $U=z=\sin(2x)$ and $V=-U'=-2\cos(2x)$. Hence the orginal ODE can be expressed as:
        \[ \frac{d}{dx}\left( \sin(2x)\frac{dy}{dx} -2\cos(2x)y \right) = x^2\sin(2x). \]
        Integrating gives:
        \[ \sin(2x)\frac{dy}{dx} - 2\cos(2x)y = \int x^2 \sin (2x) dx.\]
        So
        \[ \frac{dy}{dx} - 2\cot(2x)y = \text{cosec}(2x) \int x^2 \sin(2x)dx.\]
        
        Solving this firsrt order ODE using the integrating factor $\mu(x) = \text{cosec}(2x)$ gives:
        \[ \frac{d}{dx} \left[ \text{cosec}(2x)y \right] = \text{cosec}^2(2x) \int x^2 \sin(2x) dx \]
        Then $y(x) = \sin(2x) \int \left( \text{cosec}^2(2x)\int x^2 \sin(2x) dx \right) dx$ which yields:

        \[ y(x) = c_1 \sin(2x) + c_2 \cos(2x) - \frac{1}{8} + \frac{1}{4}x^2. \]
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}
        Find the adjoint of 
        \[ L[y] = (x^2 - x)y'' + (2x^2 + 4x - 3)y' + 8xy = 1 \]
        and use this to find the general solution.
    \end{exmp}
    \begin{sol}
        Multiply the ODE by $z(X)$:
        \[(x^2-x)zy'' + (2x^2-4x-4)zy' + 8xzy = z.\]
        We seek to express the LHS in exact form, that is:
        \begin{align*}
            (x^2-x)zy''+(2x^2+4x-3)zy' + 8xzy &= \left( Uy' + Vy\right)' \\
            &= Uy'' + (U'+V)y' + V'y,
        \end{align*}
        where $z(x)$ is the integrating factor.
        Equating coeffecients of $y'', y' $ and $y$ gives:
        \[ \begin{matrix}
            U=(x^2-x)z & \quad (\text{i}) \\
            U + V = ( 2x^2 + 4x - 3)z & \quad (\text{ii}) \\
            V' = 8xz & \quad (\text{iii}) \\
        \end{matrix} \]
        From differentiating (ii) we have
        \[ U'' + V' = z'(2x^2 + 4x - 3) + z(4x+4) \quad \text{(iv)} \]
        Then from (i) and (iii) we have
        \[ U''+V' = z'(x^2-x) + 2z'(2x-1)+2z+8zx. \quad \text{(v)} \]
        So (v) minus (iv) gives
        \[ z''(x^2-x)-z'(2x^2-1) + z(4x-2) = 0. \]
        Thus $\bar{L} = (x^2-x)\frac{d^2}{dx^2} - (2x^2-1)\frac{d}{dx} + (4x-2) $ is the adjoint operator.


        We can solve $\bar{L}[z] = 0$ to find $z(x)$. Since the ODE is similar to those of Chapter 2 and since we only need one solution rather than a general solution to $\bar{L}[z] = 0$ we try $z(x)=x^r$.

    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}
    

        Thus $\bar{L}[z] = 0$ gives:
        \[ (x^2-x)r(r-1)x^{r-2} - rx^{r-1}(2x^2-1)+x^r(4x-2) = 0. \]
        Equating coefficients of powers of $x$ we have:
        \begin{align*}
            x^r&: \quad r(r-1)-2=0 \\
            x^{r-1}&: \quad r(r-1)+r=0 \\
            x^{r+1}&: \quad 4-2r = 0. \\
        \end{align*}
        So $r=2$ satisfies all three of these equations so $x^2$ is a solution to $\bar{L}[z] = 0$. (Check this).

        So
        \begin{align*}
            U &= z(x^2-x) = x^4 - x^3 \\
            V &= U' + z(2x^2+4x-3) = 2x^4. \\
        \end{align*}
        Hence $(Uy' + Vy)' = z(x)$ yields
        \[ \left[ (x^4-x^3)y' + 2x^4y \right]' = x^2 \]
        Integrating with respect to $x$ gives: $(x^4-x^3)y' +2x^4y=\frac{x^3}{3}+c$.
        This is a first order linear ODE we can solve using integrating factors, $\mu(x) = e^{2x}(x-1)^2$. Then the ODE becomes:
        \[ ye^{2x}(x-1)^2 = \frac{1}{3} \int (x-1)e^{2x}dx + c_1 \int \frac{e^{2x}(x-1)}{x^3} dx + c_2. \]
        Thus the general solution to the original ODE is:
        \[ y(x) = \frac{1}{(x-1)^2} \left( \frac{x}{6}+\frac{1}{4}+\frac{c_1}{x^2} + c_2e^{-2x} \right). \]


\end{tcolorbox}

\subsection{Self-adjoint Operators}

\begin{definition}
    A linear operator $L$ is \textit{self-adjoint} if $L=\bar{L}$, where $\bar{L}$ is the adjoint of $L$.
\end{definition}

\begin{tcolorbox}
    \begin{exmp}[Legendre's equation]
        Let $L=(1-x^2)\frac{d^2}{dx^2} - 2x\frac{d}{dx} + \lambda $ be a linear operator. Show the operator is self-adjoint.
    \end{exmp}
    \begin{sol}
        $L[y] = (1-x^2)\frac{d^2y}{dx^2} - 2x\frac{dy}{dx} + \lambda y$, so multiplying this by $z(x)$ gives
        \begin{align*}
            (1-x^2)zy'' -2xzy' + \lambda z y &= \left( Uy' + Vy\right)' \\
            &= Uy'' + (U'+V)y' + V'y,
        \end{align*}
        Hence
        \[ \begin{matrix}
            U=(1-x^2)z & \quad (\text{i}) \\
            U + V = -2xz & \quad (\text{ii}) \\
            V' = \lambda z  & \quad (\text{iii}) \\
        \end{matrix} \]
        So differentiating (ii) gives:
        \[ U''+V' = -2z-2xz' \quad \text{(iv)}\]
        From (i) and (iii) we have
        \[ U'' + V' = z''(1-x^2) - z'4x-1z + \lambda z \quad \text{(v)}. \]
    \end{sol}
\end{tcolorbox}
\begin{tcolorbox}
        Thus (v) minus (iv) gives
        \[ z''(1-x^2) - z'(2x) + \lambda z = 0\]
        Hence $\bar{L} = (1-x^2)\frac{d^2}{dx^2} -2x \frac{d}{dx} + \lambda $ is the adjoint operator and we see $L=\bar{L}$ so $L$ is self-adjoint.
\end{tcolorbox}

Not every equation can be made exact, but they can be written in self-adjoint form. Self adjoint operators have useful properties which help to solve boundary value problems (Chapter 4). The self adjoint form of an equation can be put in canonical form which is generally easier to solve.

\begin{thm}
    A necessary condition for the operator
    \[ L = a(x) \frac{d^2}{dx} + b(x) \frac{d}{dx} + c(x) \]
    to be self adjoint is that $a'(x) = b(x)$. The self adjoint operator can be written in the general form
    \[ L = \frac{d}{dx} \left[ a(x) \frac{d}{dx} \right] + c(x) .\]
\end{thm}
\begin{proof}
    By equation \eqref{eq:adjointOfOperator}, $\bar{L}[y] = \frac{d^2}{dx^2} (ay) - \frac{d}{dx}(by) + (cy)$, so on expanding we have:
    \[ \bar{L}[y] = ay'' + (2a'-b)y' + (a''-b'+c)y.\]
    $\bar{L}=L$ if the following are satisfied
    \begin{align*}
        2a'-b = b &\implies a' = b \\
        a'' - b' + c = c &\implies a'' = b' \\
    \end{align*}
    Hence $L$ is self adjoint if $a'=b$. If $L$ is self adjoint it can be written as:
    \[ L[y] = ay'' + a'y + cy = \frac{d}{dx} \left( a\frac{dy}{dx} \right) + cy. \]
\end{proof}

\begin{thm}\label{thm:writeOperatorSelfAdjoin}
    Any linear operator may be written in self adjoint form. Given $L[y] = a(x)y'' + b(x)y' + c(x)y = f(x)$, the self adjoint form is 
    \[\hat{L}[y] = \frac{d}{dx}\left(P(x) \frac{dy}{dx} \right) + Q(x)y = H(x)\]
    where $P(x) = \exp\left(\int \frac{a(x)}{b(x)} dx \right), Q(x) = P(x)\frac{c(x)}{a(x)}, H(x)=P(x)\frac{f(x)}{a(x)}.$
\end{thm}

\begin{proof}
    Given $a(x)y'' + b(x)y' + c(x)y = f(x)$, dividing by $a(x)$ gives
    \[ y'' + \frac{b}{a} y' + \frac{c}{a}y = \frac{f}{a}. \]
    Multiplying the above equation by
    \[ P(x) = \exp\left(\int \frac{b}{a} dx \right) \]

    (Note: $y'' + \frac{b}{a}y' = Y' + \frac{b}{a}Y$, $P(x)$ is ``like'' an integrating factor for this ODE in $Y$).

    The ODE is then
    \[ \frac{d}{dx} \left(P(x)\frac{dy}{dx} \right) + P(x)\frac{c(x)}{a(x)}y  = \frac{f(x)}{a(x)}P(x).\]
    Letting $Q(x) = P(x)\frac{c(x)}{a(x)}$ and $H(x)=\frac{f(x)}{a(x)}P(x)$ we arrive at the desired form.
\end{proof}

\begin{tcolorbox}
    \begin{exmp}
        Put the differential equation $y^y - xy' -2x=0$ in self adjoint form.
    \end{exmp}
    \begin{sol}
        Let $P(x) = \exp\left(-\int x dx\right) = \exp(-\frac{x^2}{2}).$ Then
        \[ \frac{d}{dx}\left( \frac{dy}{dx} \exp\left(\frac{-x^2}{2}\right) \right) - 2y\exp\left(\frac{-x^2}{2}\right) = 0\]
        is the self-adjoint form.
    \end{sol}
\end{tcolorbox}

\subsection{Canonical Form}

\begin{thm}
    Any linear ODE, $L[y]=f$ can be written in canonical form:
    \[ \frac{d^2y}{dt^2} + \tilde{Q}(t)y = \tilde{H}(t). \]
\end{thm}

\begin{proof}
    Given $a(x)y'' + b(x)y' + c(x)y = f(x)$ then by Theorem \ref{thm:writeOperatorSelfAdjoin} we can put the equation in self adjoint form
    \begin{equation}\label{eq:canonicalformproofeq}
        \frac{d}{dx}\left[ P(x)\frac{dy}{dx} \right] + Q(x)y = H(x).
    \end{equation}
    We want to change variables so that $P(x)\frac{dy}{dx} = \frac{dy}{dt}$. By the chain rule $\frac{dy}{dt} = \frac{dy}{dx}\frac{dx}{dt}$, so $P(x) = \frac{dx}{dt}$. Hence $\int \frac{1}{P(x)} dx = t$ is the change of variables we are seeking.

    Multiplying equation \eqref{eq:canonicalformproofeq} by $P(x)$ and changing variables from $x$ to $t$ gives:
    \[ P\frac{d}{dx}\left[ P \frac{dy}{dx} \right] + PQy = Ph\]
    i.e. 
    \[\frac{d}{dt}\left[ \frac{dy}{dx} \right] + \tilde{Q}(t)y = \tilde{H}(t) \]
    where $\tilde{Q}(t) = P(x(t))Q(x(t)) $ and $\tilde{H}(t) = P(x(t))H(x(t))$ with $x(t)$ determined by $t=\int \frac{1}{P(x)} dx$.
\end{proof}

\begin{tcolorbox}
    \begin{exmp}
        Put Bessel's equation $x^2y'' + xy' + (x^2-p^2)y = 0$ into
        \begin{enumerate}[(a)]
            \item self adjoint form,
            \item canonical form.
        \end{enumerate}
    \end{exmp}
    \begin{sol}
        Divide the ODE by $x^2$: $y'' + \frac{1}{x}y' + \frac{(x^2-p^2)}{x^2}y = 0$.

        Multiplying by the factor $P(x) = \exp\left(\int\frac{1}{x} dx \right) = x$ gives:
        \begin{equation}\tag{\text{Self Adjoint Form}}
            \frac{d}{dx}\left(x \frac{dy}{dx} \right) + \frac{(x^2-p^2)}{x}y = 0
        \end{equation}

        Let $x\frac{dy}{dx} = \frac{dy}{dt}$, so comparing to $\frac{dy}{dt} = \frac{dy}{dx} \frac{dx}{dt}$ we have $\frac{dx}{dt} = x$, hence $t=\ln x$ or $x = \exp(t)$.
        Multiplying the self adjoint form by $P(x) = x$ gives:
        \[ x\frac{d}{dx}\left[ x \frac{dy}{dx} \right] + (x^2-p^2)y = 0 \]
        and changing variables from $x$ to $t$
        \begin{equation}\tag{\text{Canonical Form}}
            \frac{d^2y}{dt^2} + (e^{2t}-p^2)y = 0.
        \end{equation}
    \end{sol}
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}[Notice this is the same as Ex Sheet 4, Q6]
        Put the equation
        \[ 4xy'' + 2y' = 0 \]
        into canonical form, then solve the equation.
    \end{exmp}
    \begin{sol}
        Dividing the ODE by $4x$ gives $y'' + \frac{1}{2x}y' + \frac{1}{4x}y = 0$.
        Multiplying the ODE by $P(x)=\exp\left(\int \frac{1}{2x} dx \right) = x^{\frac{1}{2}}$.
        \begin{equation}\tag{\text{Self Adjoint Form}}
            \frac{d}{dx} \left( x^{\frac{1}{2}} \frac{dy}{dx} \right) + \frac{x^{\frac{1}{2}}}{4x} y = 0.
        \end{equation}
        Multiplying by $P(x)$ again
        \[ x^{\frac{1}{2}} \frac{d}{dx}\left( x^{\frac{1}{2}} \frac{dy}{dx} \right) + \frac{1}{4}y = 0. \]

        Changing variables so that $x^{\frac{1}{2}}\frac{dy}{dx} =\frac{dy}{dt}$ hence $\frac{dx}{dt} = x^{\frac{1}{2}}$. So $t = \int x^{\frac{-1}{2}} dx = 2x^{\frac{1}{2}}$. Thus $\frac{d^2y}{dt^2} + \frac{1}{4}y = 0$ which is a constant coefficient ODE with solutions of the form $e^{mt}$. This leades to the auxiliary equation $m^2 + \frac{1}{4} = 0$, so $m = \pm \frac{i}{2} $.

        The general solution is then $y(t) = c_1\sin\left(\frac{t}{2}\right) + c_2\cos\left(\frac{t}{2}\right).$

        Rewriting in the original $x$ variable we have:
        \[ y(x) = c_1\sin(\sqrt{x}) + c_2\cos(\sqrt{x}). \]
    \end{sol}
\end{tcolorbox}

\begin{tcolorbox}
    \begin{exmp}
        Solve the differential equation $4(1-x^2)y'' - 4xy' + y = 0$ using a canonical substitution.
    \end{exmp}
    \begin{sol}
        Let $P(x) = \exp\left(-\int\frac{x}{1-x^2} dx\right) = \exp\left( \frac{1}{2} \int \frac{-2x}{1-x^2} dx \right) = (1-x^2)^{\frac{1}{2}}$.
        So multiplying the ODE by $P(x)$ gives the self adjoint formm of the ODE as:\
        \begin{equation}\label{eq:aSelfAdjointExample}
            \frac{d}{dx}\left( \sqrt{1-x^2} \frac{dy}{dx} \right) = \frac{1}{\sqrt{1-x^2}}\frac{y}{4} = 0.
        \end{equation}
        To write this in canoncial form let $\sqrt{1-x^2}\frac{dy}{dx} = \frac{dy}{dt}$, that is $\frac{dx}{dt} = \sqrt{1-x^2}$ and so $t=\int \frac{dx}{\sqrt{1-x^2}} = \sin^{-1}(x)$ or $\sin(t) = x$. Hence equation \eqref{eq:aSelfAdjointExample} becomes:
        \[\frac{d^2y}{dt^2} + \frac{1}{4}y = 0.\]
        This has solution $y(t) = c_1\sin\left(\frac{t}{2}\right) + c_2 \cos \left( \frac{t}{2} \right), \quad c_1,c_2 \in \mathbb{R}$.

        In the original $x$ coordinate the solution is
        \[ y(x) = c_1\sin \left(\frac{1}{2} \sin^{-1}(x) \right) + c_2 \cos\left(\frac{1}{2}\sin^{-1}(x)\right). \]
    \end{sol}
\end{tcolorbox}

\pagebreak
\section{Boundary Value Problems and Sturm-Liouville Theory}

Boundary value problems (BVPs) for differential equations have boundary conditions specified at two different points $x=a$ and $x=b$ in contrast to initial value problems (IVPs) where the conditions are specified at some point $x=a$. As a result solutions to BVPs have very different properties to solutions to IVPS. See Figure \ref{fig:IVPvsBVP}.

\begin{figure}[h]
    \centering
    \includegraphics{images/IVPvsBVP.png}
    \caption{An IVP versus a BVP.}
    \label{fig:IVPvsBVP}
\end{figure}

\begin{definition}
    A two point boundary value problem is given by the second order ODE
    \[ L[y] = a(x)y'' + b(x)y' + c(x)y = f(x) \]
    subject to the unmixed homogenenous boundary condition:
    \begin{align*}
        \alpha_1 y(a) + \alpha_2 y'(a) &= 0 \\
        \beta_1 y(b) + \beta_2 y'(b) &= 0 \\
    \end{align*}
    $\alpha_i,\beta_i \in \mathbb{R}$ and $a,b,c,f,y \in C[a,b]$.
\end{definition}

\begin{remark}
    $C[a,b]$ is the set of all continuous functions $y: [a,b] \to \mathbb{R}$ and forms a vector space.
    $C^2[a,b]$ refers to the functions also being twice differentiable.
\end{remark}

\begin{itemize}
\item For $\alpha_1=\beta_1=0$ we have \colorboxed{orange}{\text{Neumann boundary conditions}} such that $y'(a)=y'(b)=0$.

\item For $\alpha_2=\beta_2=0$ we have \colorboxed{orange}{\text{Dirichlet boundary conditions}} such that $y(a)=y(b)=0$.

\item Another type of boundary condition which is often encountered is \colorboxed{orange}{\text{periodic boundary conditions}} where $y(a)=y(b)$ and $y'(a)=y'(b)$. Boundary value problems with periodic boundary conditions have to be handled differently than the above homogenenous conditions.
\end{itemize}

We can solve some BVPs using eigenvalue expansions.

\begin{definition}
    The \textit{eigenvalue} $\lambda$ and \textit{eigenfunctions} $\Phi$
\end{definition}

\subsection{Inner Product Spaces}

\subsection{Orthogonality and Reality}

\subsection{Eigenfunction Expansion Method}

\section{Green's Functions}

\subsection{Generalised Functions}

\subsection{Method of Green's Functions}

\subsection{Construction of the Green's Function}

\subsubsection{Inhomogeneous Boundary Conditions}

\subsubsection{Application of Green's functions to initial value problems}

\section{First Order Partial Differential Equations}

\subsection{First Order Constant Coefficient Linear PDEs}

\subsubsection{Geometric Method}

\subsection{Variable Coefficient Strictly Linear PDEs}

\subsubsection{Method of Characteristics}

\subsubsection{Particular Solutions}

\subsection{Quasilinear PDEs}

\section{Second Order Partial Differential Equations}

\subsection{Classification of Second Order PDEs}

\subsubsection{Hyperbolic Equations}

\subsubsection{Parabolic Equations}

\subsubsection{Elliptic Equations}

\subsection{Solutions of BVPs by Separation of variables}



\end{document}
